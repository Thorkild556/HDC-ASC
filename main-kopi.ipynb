{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import HDcompute as hd\n",
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "output_path = \"speech_commands_v0.02.tar.gz\"\n",
    "extract_dir = \"speech_commands\"\n",
    "\n",
    "\n",
    "# Define the 10 core words to include\n",
    "CORE_WORDS = {'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'}\n",
    "\n",
    "# Download the dataset\n",
    "if not os.path.exists(output_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "else:\n",
    "    print(\"Dataset already downloaded.\")\n",
    "\n",
    "# Extract the dataset\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(output_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# Parameters\n",
    "DATA_DIR = extract_dir  # Path to the dataset\n",
    "DATA_DIR = os.path.abspath(extract_dir)  # Convert to absolute path\n",
    "\n",
    "TARGET_SR = 16000  # Sampling rate\n",
    "FIXED_LENGTH = TARGET_SR  # 1 second of audio at 16kHz\n",
    "\n",
    "# Function to process a single audio file\n",
    "def process_audio(file_path, target_sr=TARGET_SR, fixed_length=FIXED_LENGTH):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "\n",
    "        # Normalize amplitude\n",
    "    if np.max(np.abs(audio)) > 0:  # Avoid division by zero\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    # Adjust length (truncate or pad with zeros)\n",
    "    if len(audio) > fixed_length:\n",
    "        audio = audio[:fixed_length]\n",
    "    elif len(audio) < fixed_length:\n",
    "        audio = np.pad(audio, (0, fixed_length - len(audio)), mode='constant')\n",
    "    return audio\n",
    "\n",
    "# Load all signals and labels for the 10 core words\n",
    "def load_signals(data_dir=DATA_DIR, core_words=CORE_WORDS):\n",
    "    signals = []\n",
    "    labels = []\n",
    "    label_map = {label: idx for idx, label in enumerate(sorted(core_words))}\n",
    "    for label in core_words:\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(label_dir, file)\n",
    "                    signal = process_audio(file_path)\n",
    "                    signals.append(signal)\n",
    "                    labels.append(label_map[label])\n",
    "    return np.array(signals), np.array(labels), label_map\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "signals, labels, label_map = load_signals()\n",
    "\n",
    "# Print shape of data\n",
    "print(f\"Signals shape: {signals.shape}\")  # (num_samples, fixed_length)\n",
    "print(f\"Labels shape: {labels.shape}\")    # (num_samples,)\n",
    "print(f\"Label map: {label_map}\")\n",
    "\n",
    "# Save signals, labels, and label_map\n",
    "with open(\"speech_commands_signals_core_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump((signals, labels, label_map), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load it back later\n",
    "with open(\"speech_commands_signals_core_words.pkl\", \"rb\") as f:\n",
    "    signals, labels, label_map = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, low=300, high=8000, sr=16000):\n",
    "    \"\"\"\n",
    "    Apply a band-pass filter to focus on speech frequencies.\n",
    "    Args:\n",
    "        signal (torch.Tensor): Audio signal of shape (num_samples, signal_length).\n",
    "        low (int): Lower cutoff frequency in Hz.\n",
    "        high (int): Upper cutoff frequency in Hz.\n",
    "        sr (int): Sampling rate in Hz.\n",
    "    Returns:\n",
    "        torch.Tensor: Filtered audio signal.\n",
    "    \"\"\"\n",
    "    from torchaudio.functional import lowpass_biquad, highpass_biquad\n",
    "    signal = highpass_biquad(signal, sr, low)\n",
    "    signal = lowpass_biquad(signal, sr, high)\n",
    "    return signal\n",
    "signals = torch.tensor(signals, dtype=torch.float32)  # Shape: (num_samples, signal_length)\n",
    "signals = bandpass_filter(signals)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Print shape of data\n",
    "print(f\"Signals shape: {signals.shape}\")  # (num_samples, fixed_length)\n",
    "print(f\"Labels shape: {labels.shape}\")    # (num_samples,)\n",
    "print(f\"Label map: {label_map}\")\n",
    "print(np.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "labels = torch.tensor(labels, dtype= torch.int32)\n",
    "print(torch.unique(labels))\n",
    "print(signals.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(signals.shape)\n",
    "\n",
    "signals = torch.tensor(signals, dtype=torch.float32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters for MFCC\n",
    "sr = 16000  # Sampling rate\n",
    "n_mfcc = 13  # Number of MFCC coefficients\n",
    "n_fft = 1300  # FFT window size\n",
    "hop_length = 1100  # Hop size \n",
    "\n",
    "\n",
    "# Torchaudio MFCC transform\n",
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=sr,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\"n_fft\": n_fft, \"hop_length\": hop_length, \"n_mels\": 40}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_mfccs_torchaudio(signals, mfcc_transform):\n",
    "    \"\"\"\n",
    "    Compute MFCCs, delta, and delta-delta features for a batch of signals using torchaudio and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        signals (list or ndarray): List or array of signals, each with shape (signal_length,).\n",
    "        mfcc_transform (torchaudio.transforms.MFCC): MFCC transformation object.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_samples, time_frames, n_mfcc * 3),\n",
    "                      containing MFCCs, deltas, and delta-deltas.\n",
    "    \"\"\"\n",
    "    # Convert the list of numpy arrays to a single PyTorch tensor\n",
    "    signal_tensor = torch.tensor(signals, dtype=torch.float32)  # Shape: (num_samples, signal_length)\n",
    "\n",
    "\n",
    "    # Apply the MFCC transform to the batch\n",
    "    mfcc = mfcc_transform(signal_tensor)  # Shape: (num_samples, n_mfcc, time_frames)\n",
    "    \n",
    "    # Swap axes to make the shape (num_samples, time_frames, n_mfcc)\n",
    "    mfcc = mfcc.permute(0, 2, 1)  # Shape: (num_samples, time_frames, n_mfcc)\n",
    "\n",
    "    # Compute delta features using PyTorch convolution for each MFCC channel independently\n",
    "    delta_filter = torch.tensor([-1, 0, 1], dtype=torch.float32).view(1, 1, -1).to(mfcc.device)  # Filter for first-order derivative\n",
    "\n",
    "    # Reshape for convolution: combine batch and MFCC channels\n",
    "    mfcc_reshaped = mfcc.permute(0, 2, 1).reshape(-1, 1, mfcc.shape[1])  # Shape: (num_samples * n_mfcc, 1, time_frames)\n",
    "\n",
    "    # Apply convolution for delta\n",
    "    delta = F.conv1d(mfcc_reshaped, delta_filter, padding=1).reshape(mfcc.shape[0], mfcc.shape[2], -1).permute(0, 2, 1)\n",
    "\n",
    "    # Apply convolution again for delta-delta\n",
    "    delta_delta = F.conv1d(delta.reshape(-1, 1, mfcc.shape[1]), delta_filter, padding=1).reshape(mfcc.shape[0], mfcc.shape[2], -1).permute(0, 2, 1)\n",
    "\n",
    "    # Concatenate MFCC, delta, and delta-delta features along the last dimension\n",
    "    features = torch.cat([mfcc, delta, delta_delta], dim=2)  # Shape: (num_samples, time_frames, n_mfcc * 3)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Compute MFCC, Delta, and Delta-Delta features for the dataset\n",
    "print(\"Computing MFCCs, Delta, and Delta-Delta features for the dataset using torchaudio...\")\n",
    "mfcc_delta_features = compute_mfccs_torchaudio(signals, mfcc_transform)\n",
    "\n",
    "print(\"Feature shape:\", mfcc_delta_features.shape)  # Should be (num_samples, time_frames, n_mfcc * 3)\n",
    "\n",
    "# Save the computed features along with labels\n",
    "with open(\"speech_commands_mfcc_delta_features_torchaudio.pkl\", \"wb\") as f:\n",
    "    pickle.dump((mfcc_delta_features, labels, label_map), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with delta features\n",
    "import pickle\n",
    "# Load it back\n",
    "with open(\"speech_commands_mfcc_delta_features_torchaudio.pkl\", \"rb\") as f:\n",
    "    mfcc_features, labels, label_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC features computed and saved.\n",
      "MFCC Features shape: torch.Size([38546, 15, 39])\n",
      "Labels shape: (38546,)\n",
      "{'down': 0, 'go': 1, 'left': 2, 'no': 3, 'off': 4, 'on': 5, 'right': 6, 'stop': 7, 'up': 8, 'yes': 9}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MFCC features computed and saved.\")\n",
    "print(f\"MFCC Features shape: {mfcc_features.shape}\")  # Shape: (num_samples, n_mfcc)\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2697)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(mfcc_features[11235][9][8])\n",
    "print(labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "torch.Size([36618, 15, 39]) torch.Size([36618])\n",
      "torch.Size([1928, 15, 39]) torch.Size([1928])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create a subset of 5000 signals\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# subset_size = 5000\n",
    "# indices = torch.randperm(len(labels))[:subset_size]\n",
    "\n",
    "# # # Sort the indices\n",
    "# sorted_indices = torch.sort(indices).values\n",
    "\n",
    "# labels = torch.tensor(labels, dtype=torch.int32).to(device)\n",
    "# mfcc_features = torch.tensor(mfcc_features, dtype=torch.float32).to(device)\n",
    "\n",
    "# subset_mfcc_features = mfcc_features[sorted_indices]\n",
    "# subset_labels = labels[sorted_indices]\n",
    "\n",
    "# print(subset_mfcc_features.shape)\n",
    "\n",
    "\n",
    "# # Define the train-test split ratio for subset\n",
    "# train_ratio = 0.80\n",
    "# train_size = int(train_ratio * subset_size)\n",
    "# test_size = subset_size - train_size\n",
    "\n",
    "# train_dataset, test_dataset = random_split(list(zip(subset_mfcc_features, subset_labels)), [train_size, test_size])\n",
    "\n",
    "\n",
    "size = labels.shape[0]\n",
    "# Define the train-test split ratio for full length\n",
    "train_ratio = 0.95\n",
    "train_size = int(train_ratio * size)\n",
    "test_size = size - train_size\n",
    "\n",
    "\n",
    "# Create the train-test split\n",
    "train_dataset, test_dataset = random_split(list(zip(mfcc_features, labels)), [train_size, test_size])\n",
    "\n",
    "# Separate the features and labels for train and test sets\n",
    "train_mfcc_features, train_labels = zip(*train_dataset)\n",
    "test_mfcc_features, test_labels = zip(*test_dataset)\n",
    "\n",
    "# Convert to tensors\n",
    "train_mfcc_features = torch.stack(train_mfcc_features).to(device)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.int32).to(device)\n",
    "test_mfcc_features = torch.stack(test_mfcc_features).to(device)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.int32).to(device)\n",
    "\n",
    "print(train_mfcc_features.shape, train_labels.shape)\n",
    "print(test_mfcc_features.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "tensor([ 1, -1, -1,  ..., -1, -1,  1], device='mps:0', dtype=torch.int8)\n",
      "tensor([-1,  1,  1,  ...,  1,  1, -1], device='mps:0', dtype=torch.int8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_79479/186551116.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mfcc_features = torch.tensor(mfcc_features, dtype=torch.float32, device=device)\n",
      "/Users/thorkildkappel/opt/anaconda3/envs/hdcompute/lib/python3.12/site-packages/torch/functional.py:905: UserWarning: MPS: _unique2 op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performace implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Unique.mm:323.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n",
      "/Users/thorkildkappel/Desktop/5. sem/Bachelor/scripts/HDcompute.py:598: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  model[idx] = torch.sum(torch_matrix[mask], dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.48038911819458\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torchaudio\n",
    "import torch\n",
    "import pickle\n",
    "import HDcompute as hd\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#mfcc_features = mfcc_features.to(device)\n",
    "\n",
    "#normalisation takes a bunch of time\n",
    "\n",
    "vector1 = torch.tensor(hd.make_rand_vector(seed=2, size = 10000), dtype = torch.int8).to(device)\n",
    "# vector2 = torch.tensor(hd.make_rand_vector(seed=8, size = 40000), dtype = torch.int8).to(device)\n",
    "vector2 = vector1 * -1\n",
    "#vector1 = vector1 * 0\n",
    "print(vector1)\n",
    "print(vector2)\n",
    "labels = torch.tensor(labels, dtype=torch.int32, device=device)\n",
    "mfcc_features = torch.tensor(mfcc_features, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# model = hd.make_model_from_mfccs(mfcc_features, labels, vector1=vector1, vector2=vector2,\n",
    "#                                     batch_size=10, device =device,\n",
    "#                                     seed = 42, n_gram = 2, majority_vote=True, separate_signals= True,\n",
    "#                                     alpha=2)\n",
    "model = hd.make_model_from_mfccs(train_mfcc_features, train_labels, vector1=vector1, vector2=vector2, \n",
    "                                  batch_size=4, device = device,\n",
    "                                  seed = 42, n_gram = 2, majority_vote=True, separate_signals= True, \n",
    "                                  alpha=1, single_window=False, weighing = True)\n",
    "end = time.time() #111.9\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[0]) \n",
    "print(model[0].shape) \n",
    "#model = (hd.majority_vote_torch(model[0]),) + model[1:]# thats bad\n",
    "#print(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "# model = (model[0].to(\"mps\"),) + model[1:]\n",
    "# test_mfcc_features = test_mfcc_features.to(\"mps\")\n",
    "preds = hd.predict(model, test_mfcc_features, majority_vote=False, weighing = True)\n",
    "\n",
    "#preds = hd.predict_vote(model, test_mfcc_features).to(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.count_nonzero(preds == labels))\n",
    "print(\"accuracy: \",round(torch.count_nonzero(preds == test_labels).item()*100/test_labels.shape[0], 4) , \"%\")\n",
    "\n",
    "# print(preds)\n",
    "# print(test_labels)\n",
    "\n",
    "#1836 for 4-gram, within sample\n",
    "#1926 for 1 gram superposition, within sample\n",
    "\n",
    "#60 % accuracy with superposition, and random seed +1 in the for loop, and n-gram = 2\n",
    "# accuracy 68-69 % with larger windows up to 2000 and no majority vote (log transform helps a bunch)\n",
    "#72% with superposition, log transformation, seperate signals, n_gram = 2.\n",
    "#76 with vector length = 20.000, \n",
    "# 74-76% with 1600 window length, 1200 skip size \n",
    "# 78 % with weighing\n",
    "\n",
    "# alpha â‰ˆ 1-3\n",
    "print(torch.count_nonzero(train_labels == 9)) \n",
    "\n",
    "#best model is with no majority vote and seperate signals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example numpy arrays (replace with your actual data)\n",
    "preds_np = np.asanyarray(preds.to(\"cpu\"))  # Replace with your predictions array\n",
    "test_labels_np = np.asanyarray(test_labels.to(\"cpu\"))  # Replace with your test labels array\n",
    "\n",
    "# Combine the arrays into a single 2D array (columns)\n",
    "data = np.column_stack((preds_np, test_labels_np))\n",
    "\n",
    "# Save to CSV\n",
    "np.savetxt(\"preds_and_test_labels.csv\", data, delimiter=\",\", header=\"preds,test_labels\", comments=\"\", fmt=\"%d\")\n",
    "\n",
    "print(\"CSV file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the CSV file back into a NumPy array\n",
    "data = np.loadtxt(\"preds_and_test_labels.csv\", delimiter=\",\", skiprows=1, dtype=np.int32)\n",
    "\n",
    "# Separate the columns\n",
    "preds_np, test_labels_np = data[:, 0], data[:, 1]\n",
    "\n",
    "print(np.count_nonzero(preds_np==test_labels_np)/preds_np.shape[0])\n",
    "\n",
    "print(\"\",preds_np[:10],\"\\n\",test_labels_np[:10])  # Print first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels_np, preds_np)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels_np, preds_np, average='weighted')  # Use 'micro', 'macro', or 'weighted' depending on needs\n",
    "\n",
    "accuracy = accuracy_score(test_labels_np, preds_np)\n",
    "\n",
    "print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Visualize confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap='viridis')  # Adjust the colormap if desired\n",
    "plt.show()\n",
    "\n",
    "# Uncomment to display the confusion matrix values\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for i in range(1, 10):\n",
    "    print(i)\n",
    "    model = hd.make_model_from_mfccs(train_mfcc_features, train_labels, vector1=vector1, vector2=vector2, batch_size=10, device =device, seed = 7, n_gram = i, majority_vote=False, separate_signals= True)\n",
    "    preds = hd.predict(model, test_mfcc_features)\n",
    "    accuracies.append(torch.count_nonzero(preds == test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multithread evaluation\n",
    "import concurrent.futures\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def train_and_evaluate(n_gram):\n",
    "    model = hd.make_model_from_mfccs(\n",
    "        train_mfcc_features, train_labels,\n",
    "        vector1=vector1, vector2=vector2, batch_size=10,\n",
    "        device=device, seed=7, n_gram=n_gram,\n",
    "        majority_vote=False, separate_signals=True\n",
    "    )\n",
    "    preds = hd.predict(model, test_mfcc_features)\n",
    "    accuracy = torch.count_nonzero(preds == test_labels).item()\n",
    "    return accuracy\n",
    "\n",
    "# Create a pool of threads to parallelize training and evaluation\n",
    "accuracies = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    n_gram_range = range(1, 10)\n",
    "    # Map each n_gram value to the train_and_evaluate function\n",
    "    results = executor.map(train_and_evaluate, n_gram_range)\n",
    "\n",
    "    # Collect results\n",
    "    accuracies = list(results)\n",
    "\n",
    "print(\"Accuracies:\", accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "print(model[0].shape)\n",
    "print(labels.shape)\n",
    "unique_labels = torch.tensor(list(OrderedDict.fromkeys(labels.tolist())), dtype=labels.dtype, device=labels.device)  \n",
    "print(unique_labels)\n",
    "print(labels[1])\n",
    "print(torch.count_nonzero(model[0][0]== 307881))\n",
    "#2462\n",
    "n = torch.count_nonzero(labels == 2)\n",
    "print(n)\n",
    "print(81*3801)\n",
    "\n",
    "print(\"LOOL\",torch.count_nonzero(model[0][0]== 307881))\n",
    "\n",
    "\n",
    "print(model[0].shape)\n",
    "\n",
    "print(torch.count_nonzero(model[0][1] == 0))\n",
    "\n",
    "\n",
    "print(torch.count_nonzero(vector1 == 1))\n",
    "print(torch.count_nonzero(vector2 == 1))\n",
    "\n",
    "\n",
    "tots = torch.tensor([-3000, 20, 3, -2, -5, -6, 3, 3, 3])\n",
    "\n",
    "tots = hd.majority_vote_torch(tots)\n",
    "print(tots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should give torch.Size([n_classes, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for encoding vectors\n",
    "batch_size = 10\n",
    "num_windows = 5\n",
    "num_mfccs = 13\n",
    "vector_length = 5\n",
    "\n",
    "# Example tensors\n",
    "mfcc_encoded_vectors = torch.rand(batch_size, num_windows, num_mfccs, vector_length)\n",
    "\n",
    "print(mfcc_encoded_vectors.shape)\n",
    "\n",
    "rand_mfcc_matrix = torch.randint(0, 2, (num_mfccs, vector_length), dtype=torch.float32) * 2 - 1\n",
    "print(rand_mfcc_matrix.shape)\n",
    "\n",
    "print(mfcc_encoded_vectors[1][1][1])\n",
    "print(rand_mfcc_matrix[1])\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for i in range(num_mfccs):\n",
    "    sum += mfcc_encoded_vectors[1][1][i]*rand_mfcc_matrix[i]\n",
    "\n",
    "print(\"sum:\", sum)\n",
    "\n",
    "# Call the function\n",
    "result = hd.bind_and_add_mfcc_vectors(mfcc_encoded_vectors, rand_mfcc_matrix)\n",
    "\n",
    "print(\"result:\", result[1][1])\n",
    "\n",
    "\n",
    "print(result.shape)  # Should print (10, 5, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity sanity check\n",
    "import torch\n",
    "import HDcompute as hd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Example usage\n",
    "M = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32).to(\"mps\")\n",
    "X = torch.tensor([[0.4, 0.5, 0.6], [1, 2, 3], [1, 1, 1]], dtype=torch.float32).to(\"mps\") # Signal vectors\n",
    "\n",
    "print(M.device)\n",
    "print(X.device)\n",
    "#12\n",
    "\n",
    "\n",
    "cos_sim_matrix = hd.cosine_similarity_matrix_torch(M, X)\n",
    "\n",
    "\n",
    "cos_sim_matrix = cos_sim_matrix.to(\"cpu\") #but whyyyy??? the gpu returns the wrong argmax for some reason.\n",
    "\n",
    "print(\"cos sim matrix\\n\",cos_sim_matrix) #20\n",
    "\n",
    "argmax = torch.argmax(cos_sim_matrix, dim = 0)\n",
    "\n",
    "print(argmax)\n",
    "\n",
    "\n",
    "\n",
    "device = M.device\n",
    "labels = labels.clone().to(device=device, dtype=torch.int32)\n",
    "unique_labels = torch.tensor(list(OrderedDict.fromkeys(labels.tolist())), dtype=labels.dtype, device=labels.device) #30\n",
    "\n",
    "\n",
    "print(unique_labels[argmax])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import HDcompute as hd\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "\n",
    "def encode_mfcc_batch_with_percentage_flip(\n",
    "    mfccs_batch, num_cols, num_mfccs, vector_length, \n",
    "    min_vector, mean_vector, max_vector, device, seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Calculate mean for each MFCC across time windows for each signal\n",
    "    mean_vals = torch.mean(mfccs_batch, dim=1, keepdim=True)  # Shape: (signals, 1, mfccs)\n",
    "\n",
    "    # Calculate range (max - mean or mean - min) and avoid division by zero\n",
    "    range_vals = torch.amax(mfccs_batch, dim=1, keepdim=True) - torch.amin(mfccs_batch, dim=1, keepdim=True) + 1e-6\n",
    "\n",
    "    # Calculate percentage deviation from the mean\n",
    "    percentages = (mfccs_batch - mean_vals) / range_vals  # Shape: (signals, windows, mfccs)\n",
    "    percentages = percentages.clamp(min=-1, max=1)  # Clamp to range [-1, 1]\n",
    "\n",
    "    # Expand percentage tensor to match hypervector dimensions\n",
    "    percentages_expanded = (percentages * 2).clamp(min=-1, max=1).unsqueeze(-1)  # Amplify influence\n",
    "\n",
    "\n",
    "    # Expand vectors to match batch dimensions\n",
    "    mean_vector_expanded = mean_vector.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, mfccs, vector_length)\n",
    "    min_vector_expanded = min_vector.unsqueeze(0).unsqueeze(0)\n",
    "    max_vector_expanded = max_vector.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Generate random values for flipping\n",
    "    random_values = torch.rand(mfccs_batch.shape[0], num_cols, num_mfccs, vector_length, device=device)\n",
    "\n",
    "    # Calculate masks for flipping towards min and max\n",
    "    flip_towards_min = percentages_expanded < 0  # True if deviation is below the mean\n",
    "    flip_towards_max = percentages_expanded > 0  # True if deviation is above the mean\n",
    "\n",
    "    # Compute absolute percentages for blending\n",
    "    abs_percentages = percentages_expanded.abs()\n",
    "\n",
    "    # Generate the final flipped vectors\n",
    "    flipped_towards_min = torch.where(\n",
    "        random_values < abs_percentages,\n",
    "        min_vector_expanded,\n",
    "        mean_vector_expanded\n",
    "    )\n",
    "    flipped_towards_max = torch.where(\n",
    "        random_values < abs_percentages,\n",
    "        max_vector_expanded,\n",
    "        mean_vector_expanded\n",
    "    )\n",
    "\n",
    "    # Combine based on masks\n",
    "    flipped_vectors = torch.where(\n",
    "        flip_towards_min,\n",
    "        flipped_towards_min,\n",
    "        flipped_towards_max\n",
    "    )\n",
    "\n",
    "    return flipped_vectors\n",
    "\n",
    "\n",
    "\n",
    "# Example dimensions\n",
    "signals = 2  # Number of signals\n",
    "windows = 81  # Number of time windows\n",
    "mfccs = 39  # Number of MFCC coefficients\n",
    "vector_length = 10000  # Hypervector dimensionality\n",
    "\n",
    "# Random input and reference vectors\n",
    "mfccs_batch = torch.rand(signals, windows, mfccs, device=device)\n",
    "min_vector = torch.tensor(hd.make_rand_vector(seed=2, size = 10000), dtype = torch.int8).to(device)\n",
    "mean_vector = torch.tensor(hd.make_rand_vector(seed=3, size = 10000), dtype = torch.int8).to(device)\n",
    "max_vector = torch.tensor(hd.make_rand_vector(seed=4, size = 10000), dtype = torch.int8).to(device)\n",
    "\n",
    "# Encode MFCCs\n",
    "encoded_hypervectors = encode_mfcc_batch_with_percentage_flip(\n",
    "    mfccs_batch, num_cols=windows, num_mfccs=mfccs, vector_length=vector_length,\n",
    "    min_vector=min_vector, mean_vector=mean_vector, max_vector=max_vector,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "print(encoded_hypervectors.shape)  # Should be (signals, windows, mfccs, vector_length)\n",
    "\n",
    "def hamming_distance(tensor1, tensor2):\n",
    "    num_matches = torch.sum(tensor1 == tensor2).item()\n",
    "    #total_elements = tensor1.numel()\n",
    "    #fraction_matches = num_matches / total_elements\n",
    "    return num_matches\n",
    "\n",
    "\n",
    "print(\"distances:\")\n",
    "print(\"max\",hamming_distance(encoded_hypervectors[0][0][0], max_vector))\n",
    "print(\"min\",hamming_distance(encoded_hypervectors[0][0][0], min_vector))\n",
    "print(\"mean\",hamming_distance(encoded_hypervectors[0][0][0], mean_vector))\n",
    "\n",
    "print(mfccs_batch[0][0][0])\n",
    "print(torch.max(mfccs_batch[0][:][0]))\n",
    "print(torch.min(mfccs_batch[0][:][0]))\n",
    "print(torch.mean(mfccs_batch[0][:][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import HDcompute as hd\n",
    "\n",
    "\n",
    "# Create a simple test signal tensor (5 windows, vector length 4)\n",
    "test_signal = torch.tensor([[\n",
    "    [1, 2, 3, 4],  # Window 1\n",
    "    [5, 6, 7, 8],  # Window 2\n",
    "    [9, 10, 11, 12],  # Window 3\n",
    "    [13, 14, 15, 16],  # Window 4\n",
    "    [17, 18, 19, 20]   # Window 5\n",
    "]], dtype=torch.float)\n",
    "\n",
    "print(hd.n_gram_encode_tensor_torch(test_signal, 3)) #4619\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test signal tensor (5 windows, vector length 4)\n",
    "test_signal = torch.tensor([[\n",
    "    [1, 2, 3, 4],  # Window 1\n",
    "    [5, 6, 7, 8],  # Window 2\n",
    "    [9, 10, 11, 12],  # Window 3\n",
    "    [13, 14, 15, 16],  # Window 4\n",
    "    [17, 18, 19, 20]   # Window 5\n",
    "]], dtype=torch.float)\n",
    "\n",
    "#print(\"test signal\\n\",test_signal)\n",
    "\n",
    "test_signal_copy = test_signal.clone()\n",
    "\n",
    "test_signal_permutated = torch.roll(test_signal, shifts=1, dims=2) # permutation\n",
    "test_signal_permutated_2 = torch.roll(test_signal, shifts=2, dims=2) # permutation \n",
    "test_signal_permutated_3 = torch.roll(test_signal, shifts=3, dims=2) # permutation \n",
    "\n",
    "# print(test_signal_permutated_3)\n",
    "# print(test_signal_permutated_2)\n",
    "# print(test_signal_permutated)\n",
    "# print(test_signal)\n",
    "\n",
    "print(test_signal_copy)\n",
    "\n",
    "test_signal_permutated[:, 1:4] *= test_signal_copy[:, 2:5]\n",
    "\n",
    "test_signal_permutated_2[:, 0:3] *= test_signal_permutated[:, 1:4] # we never change test_signal permutated_2[:, 4]\n",
    "\n",
    "\n",
    "test_signal[:, 2:5] *= test_signal_permutated[:, 1:4]\n",
    "\n",
    "test_signal[:, 1:4] *= test_signal_permutated_2[:, 0:3]\n",
    "\n",
    "\n",
    "\n",
    "print(test_signal_permutated_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model_from_torch_matrix(torch_matrix, target):\n",
    "    # Determine the number of unique labels\n",
    "    unique_labels = torch.tensor(list(OrderedDict.fromkeys(target.tolist())), dtype=target.dtype, device=target.device)\n",
    "    n_target = len(unique_labels)\n",
    "    model = torch.zeros((n_target, torch_matrix.shape[1]), dtype=torch.float32).to(torch_matrix.device)\n",
    "\n",
    "    row = 0\n",
    "\n",
    "    for i, Class in enumerate(unique_labels):\n",
    "        print(Class)\n",
    "        while row < torch_matrix.shape[0] and Class == target[row]: #until we are through the class, start adding vectors in the same class\n",
    "            model[i] += torch_matrix[row]\n",
    "            row += 1 \n",
    "        print(row)\n",
    "    return model, unique_labels\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from HDcompute import make_model_from_torch_matrix\n",
    "\n",
    "# Example data\n",
    "torch_matrix = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n",
    "target = torch.tensor([2, 2, 2, 2, 3, 3, 3, 3, 3, 5, 5, 5, 5, 0, 0, 0, 0, 3, 3, 3])\n",
    "\n",
    "# Create the model\n",
    "model, unique_labels = make_model_from_torch_matrix(torch_matrix, target)\n",
    "\n",
    "# Print the model and unique labels\n",
    "print(\"Model:\", model)\n",
    "print(\"Unique Labels:\", unique_labels)\n",
    "\n",
    "# Example usage of the model\n",
    "print(model[2])\n",
    "# model = (hd.majority_vote_torch(model[0]),) + model[1:]\n",
    "# print(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Original function\n",
    "def normalise_mfccs_original(mfccs, separate_signals = False):\n",
    "    copy_mfccs = mfccs.clone()\n",
    "    if separate_signals == False:\n",
    "        #find the min and max for each mfcc group eg. 26 values\n",
    "        for i in range(mfccs.shape[2]):     #for each mfcc coeficient\n",
    "            max = torch.max(mfccs[:, :, i]) #get the max for that coefficient\n",
    "            min = torch.min(mfccs[:, :, i]) #get the max for that coefficient\n",
    "            diff = max-min                  #calculate the difference\n",
    "            copy_mfccs[:, :, i] =  (copy_mfccs[:, :, i]-min)/diff\n",
    "\n",
    "    elif separate_signals == True:\n",
    "        for j in range (mfccs.shape[0]): #for each signal\n",
    "            for i in range(mfccs.shape[2]):     #for each mfcc coeficient\n",
    "                max = torch.max(mfccs[j, :, i]) #get the max for that coefficient\n",
    "                min = torch.min(mfccs[j, :, i]) #get the max for that coefficient\n",
    "                diff = max-min                  #calculate the difference\n",
    "                copy_mfccs[j, :, i] =  (copy_mfccs[j, :, i]-min)/diff\n",
    "\n",
    "    return copy_mfccs\n",
    "\n",
    "# Refactored function\n",
    "def normalise_mfccs_refactored(mfccs, separate_signals=False):\n",
    "    copy_mfccs = mfccs.clone()\n",
    "    if not separate_signals:\n",
    "        # Use amin and amax for multidimensional min/max\n",
    "        min_vals = torch.amin(mfccs, dim=(0, 1), keepdim=True)\n",
    "        max_vals = torch.amax(mfccs, dim=(0, 1), keepdim=True)\n",
    "        diff = max_vals - min_vals\n",
    "        copy_mfccs = (copy_mfccs - min_vals) / diff\n",
    "    else:\n",
    "        # Use amin and amax for reduction along specific dimensions\n",
    "        min_vals = torch.amin(mfccs, dim=1, keepdim=True)\n",
    "        max_vals = torch.amax(mfccs, dim=1, keepdim=True)\n",
    "        diff = max_vals - min_vals\n",
    "        copy_mfccs = (copy_mfccs - min_vals) / diff\n",
    "\n",
    "    return copy_mfccs\n",
    "\n",
    "\n",
    "# Test\n",
    "def test_normalise_mfccs():\n",
    "    # Create random test data\n",
    "    torch.manual_seed(42)\n",
    "    signals = 4000  # Number of signals\n",
    "    frames = 81   # Number of frames per signal\n",
    "    coeffs = 39   # Number of MFCC coefficients\n",
    "    mfccs = torch.rand((signals, frames, coeffs))\n",
    "\n",
    "    # Test with separate_signals=False\n",
    "    original_output = normalise_mfccs_original(mfccs, separate_signals=False)\n",
    "    refactored_output = normalise_mfccs_refactored(mfccs, separate_signals=False)\n",
    "    assert torch.allclose(original_output, refactored_output), \\\n",
    "        \"Outputs do not match for separate_signals=False!\"\n",
    "\n",
    "    # Test with separate_signals=True\n",
    "    start = time.time()\n",
    "    original_output = normalise_mfccs_original(mfccs, separate_signals=True)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "    start = time.time()\n",
    "    refactored_output = normalise_mfccs_refactored(mfccs, separate_signals=True)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "    assert torch.allclose(original_output, refactored_output), \\\n",
    "        \"Outputs do not match for separate_signals=True!\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_normalise_mfccs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdcompute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
